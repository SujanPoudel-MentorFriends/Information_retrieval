{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3286f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.engine import create_engine\n",
    "import sqlalchemy\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d62629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqlConnection:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.conn_engine = None\n",
    "\n",
    "    def _connect(self):\n",
    "        if self.conn_engine is None:\n",
    "            \n",
    "            \n",
    "            DATABASE_URL = \"mysql+pymysql://jeffreytest:SGDroid$99@192.168.1.250:3306/boomconsole_dev_server\"\n",
    "#             self.conn_engine = sqlalchemy.create_engine(\"mysql+pymysql://reader:%s@192.168.1.249:3306/boomconsole_dev_server\" % quote_plus(\"Freeschema@123\"))\n",
    "\n",
    "            self.conn_engine = sqlalchemy.create_engine(DATABASE_URL)\n",
    "\n",
    "    def fetch_table(self, table_name):\n",
    "        self._connect() \n",
    "        \n",
    "        table_data = pd.read_sql_table(table_name, self.conn_engine)\n",
    "        \n",
    "        self.close_database()\n",
    "        \n",
    "        return table_data\n",
    "    \n",
    "    def fetch_with_sql_query(self, sql_query):\n",
    "        self._connect() \n",
    "        \n",
    "        result = pd.read_sql_query(sql_query, self.conn_engine)\n",
    "        \n",
    "        self.close_database()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def fetch_one_with_sql_query(self, sql_query):\n",
    "        self._connect()\n",
    "        \n",
    "        # Execute the SQL query and read the result into a DataFrame\n",
    "        result = pd.read_sql_query(sql_query, self.conn_engine)\n",
    "\n",
    "        # Check if there is data in the DataFrame\n",
    "        if not result.empty:\n",
    "            # Access the first row (in this case, the only row if you used LIMIT 1)\n",
    "            first_row = result.iloc[0]\n",
    "\n",
    "            # Now, you can access specific columns from the row using column names\n",
    "            column_value = first_row\n",
    "\n",
    "            return column_value\n",
    "        else:\n",
    "            return 'No data found for the query.'\n",
    "        \n",
    "    def close_database(self):\n",
    "        if self.conn_engine:\n",
    "            self.conn_engine.dispose()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc9e1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class UserKnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        self.database_instance = SqlConnection()\n",
    "\n",
    "    def return_user_connections(self, user_id):\n",
    "        # Query for retrieving user connections\n",
    "        query = f\"SELECT * FROM the_connections WHERE user_id={user_id} AND order_id < 3\"\n",
    "        result = self.database_instance.fetch_with_sql_query(query)\n",
    "        return result\n",
    "    \n",
    "    # get unique concept id used in the connections in connections_table\n",
    "    def return_user_concept(self, user_id):\n",
    "        conn_df = self.return_user_connections(user_id)\n",
    "        unique_concepts = set()\n",
    "        for index, row in conn_df.iterrows():\n",
    "            of_concept = row['of_the_concepts_id']\n",
    "            type_concept = row['type_id']\n",
    "            to_concept = row['to_the_concepts_id']\n",
    "            unique_concepts.add(of_concept)\n",
    "            unique_concepts.add(type_concept)\n",
    "            unique_concepts.add(to_concept)\n",
    "        return unique_concepts\n",
    "    \n",
    "    # get concept character value and its type_id with character value from unique concept_id\n",
    "    def character_set(self, user_id):\n",
    "        concepts_list = self.return_user_concept(user_id)\n",
    "        concepts_ids = ', '.join(map(str, concepts_list))\n",
    "        query = f\"\"\"\n",
    "                    SELECT\n",
    "                        concept2.id as concept_type_id,\n",
    "                        concept2.character_value as concept_type_character,\n",
    "                        concept1.id as concept_id,\n",
    "                        concept1.character_value\n",
    "                    FROM the_concepts as concept1\n",
    "                    INNER JOIN the_concepts as concept2\n",
    "                    ON concept1.type_id = concept2.id\n",
    "                    WHERE concept1.id IN ({concepts_ids}) \n",
    "                \"\"\"\n",
    "        result = self.database_instance.fetch_with_sql_query(query)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def extract_keywords(self, question):\n",
    "        doc = nlp(question)\n",
    "        keywords = [token.text for token in doc if not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADJ']]\n",
    "        return keywords\n",
    "\n",
    "    def keywords_related_concepts(self, keyword_list, characters_df):\n",
    "        concept_list = {}\n",
    "        for index, row in characters_df.iterrows():\n",
    "            for key in keyword_list:\n",
    "                if key == row['character_value']:\n",
    "                    if key not in concept_list:\n",
    "                        concept_list[key] = []\n",
    "                    concept_list[key].append(row['concept_id'])\n",
    "                elif 'the_'+key == row['concept_type_character']:\n",
    "                    if key not in concept_list:\n",
    "                        concept_list[key] = []\n",
    "                    concept_list[key].append(row['concept_id'])\n",
    "        return concept_list\n",
    "\n",
    "    def DF_Search(self, dataframe, source, destination, visited=None, path=None):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if path is None:\n",
    "            path = []\n",
    "\n",
    "        visited.add(source)\n",
    "        path.append(source)\n",
    "\n",
    "        if source == destination:\n",
    "            return path\n",
    "\n",
    "        neigh_df = dataframe[dataframe['of_the_concepts_id'] == int(source)]\n",
    "        for _, row in neigh_df.iterrows():\n",
    "            of_id = row['of_the_concepts_id']\n",
    "            to_id = row['to_the_concepts_id']\n",
    "            if to_id not in visited:\n",
    "                new_path = self.DF_Search(dataframe, to_id, destination, visited.copy(), path.copy())\n",
    "                if new_path:\n",
    "                    return new_path\n",
    "\n",
    "    def keywords_with_source_to_destination(self, user_connections_df, concept_list):\n",
    "        final = {}\n",
    "        for key, value in concept_list.items():\n",
    "            final[key] = {}\n",
    "            for concept in value:\n",
    "                conn_df = user_connections_df[user_connections_df['to_the_concepts_id'] == int(concept)]\n",
    "                if concept not in final[key]:\n",
    "                    final[key][concept] = []\n",
    "                for _, row in conn_df.iterrows():\n",
    "                    source = row['type_id']\n",
    "                    destination = row['to_the_concepts_id']\n",
    "                    conn_df = user_connections_df[user_connections_df['type_id'] == int(source)]\n",
    "                    output = self.DF_Search(conn_df, source, destination)\n",
    "                    final[key][concept].append(output)\n",
    "        return final\n",
    "\n",
    "    def convert_id(self, dfs_result, character_df):\n",
    "        text = \"\"\n",
    "        for idx, cid in enumerate(dfs_result):\n",
    "            concept_id = cid\n",
    "            concept_type_character_value = character_df[character_df['concept_id'] == concept_id].iloc[0]['concept_type_character']\n",
    "            if text == \"\":\n",
    "                text = concept_type_character_value\n",
    "            else:\n",
    "                text = text + \" \" + concept_type_character_value\n",
    "            if idx == len(dfs_result) - 1:\n",
    "                concept_character_value = character_df[character_df['concept_id'] == concept_id].iloc[0]['character_value']\n",
    "                text = text + ' has ' + concept_character_value\n",
    "        return text\n",
    "\n",
    "    def converted_keywords(self, final, character_df):\n",
    "        converted_final = {}\n",
    "        items = []\n",
    "        for key, value in final.items():\n",
    "            converted_final[key] = {}\n",
    "            for k, v in value.items():\n",
    "                for i in v:\n",
    "                    if i != None:\n",
    "                        res = self.convert_id(i, character_df)\n",
    "                        items.append(res)\n",
    "        return items\n",
    "\n",
    "    def split_snake_and_camel_case(self, text):\n",
    "        snake_cased = re.sub(r'([a-z0-9])([A-Z])', r'\\1 \\2', text)\n",
    "        words = snake_cased.split('_')\n",
    "        return words\n",
    "\n",
    "    def split_keywords_into_words(self, keywords):\n",
    "        preprocessed_keywords = []\n",
    "        for keyword in keywords:\n",
    "            for word in keyword:\n",
    "                words = self.split_snake_and_camel_case(keyword)\n",
    "            preprocessed_keyword = \" \".join(words)\n",
    "            preprocessed_keywords.append(preprocessed_keyword)\n",
    "        return preprocessed_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12c09706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords :  ['current', 'subscription']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_retriver(question, user_id):\n",
    "    # Instantiate the class\n",
    "    user_knowledge_graph = UserKnowledgeGraph()\n",
    "    \n",
    "    user_connections_df = user_knowledge_graph.return_user_connections(user_id)\n",
    "\n",
    "    extracted_keywords = user_knowledge_graph.extract_keywords(question) # get keywords\n",
    "    print(\"Keywords : \", extracted_keywords)\n",
    "    characters_df = user_knowledge_graph.character_set(user_id)  # get characters df of the user id based on connections\n",
    "    concept_list = user_knowledge_graph.keywords_related_concepts(extracted_keywords, characters_df)  # get list of concepts of extracted keywords\n",
    "    \n",
    "    type_to_concept_path = user_knowledge_graph.keywords_with_source_to_destination(user_connections_df, concept_list) # Get each concept type id and return path from type as source and own concept as destination\n",
    "    \n",
    "    converted_path = user_knowledge_graph.converted_keywords(type_to_concept_path, characters_df)   # Get converted path from id to character\n",
    "    \n",
    "    splitted_converted_path = user_knowledge_graph.split_keywords_into_words(converted_path)\n",
    "    \n",
    "    return splitted_converted_path \n",
    "\n",
    "# question = \"phone number of me\"\n",
    "question = \"what is my current subscription?\"\n",
    "user_id = 10658\n",
    "retrieved_possible_answers = custom_retriver(question, user_id)\n",
    "retrieved_possible_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded4299",
   "metadata": {},
   "source": [
    "# Measure Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbc4574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CosineSimilarityCalculator:\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "\n",
    "    # Step 1: Preprocess the text\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()  # lowercase \n",
    "        tokens = text.split(\" \")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()   # lematize the word\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Step 2: Convert text to numerical vectors\n",
    "    def text_to_vectors(self, texts):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        return tfidf_matrix\n",
    "\n",
    "    # Step 3: Calculate cosine similarity\n",
    "    def calculate_cosine_similarity(self, tfidf_matrix_question, tfidf_matrix_answer):\n",
    "        cosine_similarity_score = cosine_similarity(tfidf_matrix_question, tfidf_matrix_answer)\n",
    "        return cosine_similarity_score[0][0]\n",
    "\n",
    "    def calculate_cosine_similarity_for_keywords(self, question, possible_answer_list):\n",
    "        print(\"Question : \", question)\n",
    "        answers = []\n",
    "        # Preprocess the question\n",
    "        preprocessed_question = self.preprocess_text(question)\n",
    "        \n",
    "        # Iterate over each possible_answer_list\n",
    "        for data in possible_answer_list:\n",
    "            # Preprocess the answer (keyword)\n",
    "            preprocessed_question = self.preprocess_text(question)\n",
    "            # print(\"Preprocessed Question : \", preprocessed_question)\n",
    "            preprocessed_answer = self.preprocess_text(data)\n",
    "\n",
    "            # Combine question and answer for fitting vectorizer\n",
    "            combined_texts = [preprocessed_question, preprocessed_answer]\n",
    "\n",
    "            # Convert combined texts to numerical vectors\n",
    "            tfidf_matrix_combined = self.text_to_vectors(combined_texts)\n",
    "\n",
    "            # Separate TF-IDF matrices for question and answer\n",
    "            tfidf_matrix_question = tfidf_matrix_combined[:1]  # Extract first row for question\n",
    "            tfidf_matrix_answer = tfidf_matrix_combined[1:]   # Extract second row for answer\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_similarity_score = self.calculate_cosine_similarity(tfidf_matrix_question, tfidf_matrix_answer)\n",
    "\n",
    "            # Store answer and cosine similarity score\n",
    "            answers.append({\n",
    "                \"question\" : question,\n",
    "                \"answer\": data,\n",
    "                \"cosine_similarity\": cosine_similarity_score\n",
    "            })\n",
    "        \n",
    "        return answers\n",
    "\n",
    "    def get_highest_cosine_similarity(self, cosine_score_list):\n",
    "        # Initialize variables to store maximum cosine similarity and corresponding answer\n",
    "        max_cosine_similarity = float('-inf')\n",
    "        answer_with_max_cosine_similarity = None\n",
    "\n",
    "        # Iterate over the list\n",
    "        for result in cosine_score_list:\n",
    "            # Retrieve cosine similarity score and corresponding answer\n",
    "            cosine_similarity = result['cosine_similarity']\n",
    "            answer = result['answer']\n",
    "\n",
    "            # Update maximum cosine similarity and corresponding answer if current score is greater\n",
    "            if cosine_similarity > max_cosine_similarity:\n",
    "                max_cosine_similarity = cosine_similarity\n",
    "                answer_with_max_cosine_similarity = answer\n",
    "\n",
    "        # Print maximum cosine similarity and corresponding answer\n",
    "        print(\"Maximum Cosine Similarity Score:\", max_cosine_similarity)\n",
    "        print(\"Answer with Maximum Cosine Similarity:\", answer_with_max_cosine_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e6c35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  phone number of me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mbaal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 1 phones 0 the phone has 9811111111',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 1 phones 0 the phone has 9811111111',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'user details the phone has 9866575320',\n",
       "  'cosine_similarity': 0.22028815056182974},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 0 phones 0 the phone has 9822222222',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 0 phones 0 the phone has 9822222222',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information phone 0 the phone has 79644456434344',\n",
       "  'cosine_similarity': 0.2912194185636897},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 1 phones 0 the phone has 97878454632265',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data company Information contact Person 0 phones 0 the phone has 98974343435444',\n",
       "  'cosine_similarity': 0.26055567105626243},\n",
       " {'question': 'phone number of me',\n",
       "  'answer': 'boomgpt crm data phone 0 the number has 9811111111',\n",
       "  'cosine_similarity': 0.4494364165239822}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the cosine similarity calculator\n",
    "calculator = CosineSimilarityCalculator()\n",
    "\n",
    "# Calculate cosine similarity for keywords\n",
    "cosine_score_list = calculator.calculate_cosine_similarity_for_keywords(question, retrieved_possible_answers)\n",
    "cosine_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd1283d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cosine Similarity Score: 0.4494364165239822\n",
      "Answer with Maximum Cosine Similarity: boomgpt crm data phone 0 the number has 9811111111\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and print the highest cosine similarity score\n",
    "calculator.get_highest_cosine_similarity(cosine_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd0149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
